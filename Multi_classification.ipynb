{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqIgwZ5ccfMb"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "# Imports and constants\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.datasets import ImageFolder\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from torch.utils.data import Subset\n",
        "import random\n",
        "\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.0001\n",
        "EPOCHS = 15\n",
        "\n",
        "class CrossoutDataset(Dataset):\n",
        "    def __init__(self, folder_root, transform=None):\n",
        "        self.base = ImageFolder(root=folder_root, transform=transform)\n",
        "        self.clean_idx = self.base.class_to_idx['CLEAN']\n",
        "        self.mixed_idx = self.base.class_to_idx['MIXED']\n",
        "        self.cross_types = [c for c in self.base.classes if c not in ('CLEAN', 'MIXED')]\n",
        "        self.type2idx = {c: i for i, c in enumerate(self.cross_types)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, class_idx = self.base[idx]\n",
        "        folder_name = self.base.classes[class_idx]\n",
        "        is_crossed = 0 if class_idx == self.clean_idx else 1\n",
        "        type_label = self.type2idx.get(folder_name, 0)\n",
        "        return img, is_crossed, type_label\n",
        "\n",
        "# Data loading function\n",
        "def get_data_loaders(batch_size, data_root, sample_fraction=0.2, num_workers=4):\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.Resize((224, 224)),  # For ResNet or larger models\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    train_dir = os.path.join(data_root, \"train/images\")\n",
        "    val_dir = os.path.join(data_root, \"val/images\")\n",
        "    test_dir = os.path.join(data_root, \"test/images\")\n",
        "\n",
        "    train_dataset = CrossoutDataset(train_dir, transform=train_transform)\n",
        "    val_dataset = CrossoutDataset(val_dir, transform=test_transform)\n",
        "    test_dataset = CrossoutDataset(test_dir, transform=test_transform)\n",
        "\n",
        "    # Reduce training dataset size for faster runs\n",
        "    if sample_fraction < 1.0:\n",
        "        train_size = int(len(train_dataset) * sample_fraction)\n",
        "        indices = random.sample(range(len(train_dataset)), train_size)\n",
        "        train_dataset = Subset(train_dataset, indices)\n",
        "\n",
        "    trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    validationloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    return trainloader, validationloader, testloader\n",
        "\n",
        "# Model definition\n",
        "from torchvision.models import resnet18\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CrossoutResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CrossoutResNet, self).__init__()\n",
        "        self.backbone = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "\n",
        "        # Freeze pretrained layers\n",
        "        # Freeze all layers first\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Then unfreeze last few layers (e.g., layer4)\n",
        "        for param in self.backbone.layer4.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "\n",
        "        # Remove original classifier (fc layer)\n",
        "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])  # [B, 512, 1, 1]\n",
        "\n",
        "        # Add custom classification heads\n",
        "        self.fc1 = nn.Linear(512, 128)\n",
        "        self.fc_bin = nn.Linear(128, 2)   # Binary: CLEAN vs CROSSED\n",
        "        self.fc_type = nn.Linear(128, 7)  # Multi-class: 7 cross types\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = x.view(x.size(0), -1)         # Flatten to [B, 512]\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc_bin(x), self.fc_type(x)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, criterion_bin, criterion_class, optimizer,\n",
        "                trainloader, validationloader, num_epochs, device, lambda_type=1.0):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss, correct_bin, correct_type, total, valid_type_total = 0, 0, 0, 0, 0\n",
        "\n",
        "        for inputs, is_crossed, type_label in tqdm(trainloader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "            inputs, is_crossed, type_label = inputs.to(device), is_crossed.to(device), type_label.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out_bin, out_type = model(inputs)\n",
        "            loss_bin = nn.CrossEntropyLoss()(out_bin, is_crossed)\n",
        "            mask = (is_crossed == 1)\n",
        "            loss_type = nn.CrossEntropyLoss()(out_type[mask], type_label[mask]) if mask.any() else torch.tensor(0.0, device=device)\n",
        "            loss = loss_bin + lambda_type * loss_type\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total += is_crossed.size(0)\n",
        "            correct_bin += (out_bin.argmax(1) == is_crossed).sum().item()\n",
        "            if mask.any():\n",
        "                correct_type += (out_type[mask].argmax(1) == type_label[mask]).sum().item()\n",
        "                valid_type_total += mask.sum().item()\n",
        "\n",
        "        bin_acc = correct_bin / total\n",
        "        type_acc = correct_type / valid_type_total if valid_type_total else 0.0\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Binary Acc: {bin_acc:.4f}, Type Acc: {type_acc:.4f}\")\n",
        "        train_losses.append(total_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss, val_correct_bin, val_correct_type, val_total, val_type_total = 0, 0, 0, 0, 0\n",
        "        val_type_correct = {}\n",
        "        val_type_total_dict = {}\n",
        "        for i in range(len(validationloader.dataset.cross_types)):\n",
        "            val_type_correct[i] = 0\n",
        "            val_type_total_dict[i] = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, is_crossed, type_label in tqdm(validationloader, desc=f\"Validation Epoch {epoch+1}\"):\n",
        "                inputs, is_crossed, type_label = inputs.to(device), is_crossed.to(device), type_label.to(device)\n",
        "                out_bin, out_type = model(inputs)\n",
        "                loss_bin = nn.CrossEntropyLoss()(out_bin, is_crossed)\n",
        "                mask = (is_crossed == 1)\n",
        "                loss_type = nn.CrossEntropyLoss()(out_type[mask], type_label[mask]) if mask.any() else torch.tensor(0.0, device=device)\n",
        "                loss = loss_bin + lambda_type * loss_type\n",
        "                val_loss += loss.item()\n",
        "                val_total += is_crossed.size(0)\n",
        "                val_correct_bin += (out_bin.argmax(1) == is_crossed).sum().item()\n",
        "                if mask.any():\n",
        "                    preds = out_type[mask].argmax(1)\n",
        "                    labels = type_label[mask]\n",
        "                    val_correct_type += (preds == labels).sum().item()\n",
        "                    val_type_total += mask.sum().item()\n",
        "                    for t, p in zip(labels, preds):\n",
        "                        t = t.item()\n",
        "                        val_type_total_dict[t] += 1\n",
        "                        if p.item() == t:\n",
        "                            val_type_correct[t] += 1\n",
        "\n",
        "        val_losses.append(val_loss)\n",
        "        val_bin_acc = val_correct_bin / val_total\n",
        "        val_type_acc = val_correct_type / val_type_total if val_type_total else 0.0\n",
        "        print(f\"Validation - Loss: {val_loss:.4f}, Binary Acc: {val_bin_acc:.4f}, Type Acc: {val_type_acc:.4f}\")\n",
        "        print(\"Validation - Per-type Accuracy:\")\n",
        "        for i in range(len(validationloader.dataset.cross_types)):\n",
        "            acc = val_type_correct[i] / val_type_total_dict[i] if val_type_total_dict[i] > 0 else 0\n",
        "            print(f\"  {validationloader.dataset.cross_types[i]}: {acc:.4f}\")\n",
        "        train_losses = []\n",
        "    val_losses = []\n",
        "    model.train()\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate(model, testloader):\n",
        "    model.eval()\n",
        "    correct_bin, correct_type, total, valid_type_total = 0, 0, 0, 0\n",
        "\n",
        "    y_true_binary = []\n",
        "    y_pred_binary = []\n",
        "    y_true_type = []\n",
        "    y_pred_type = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, is_crossed, type_label in testloader:\n",
        "            inputs, is_crossed, type_label = inputs.to(device), is_crossed.to(device), type_label.to(device)\n",
        "            out_bin, out_type = model(inputs)\n",
        "\n",
        "            total += is_crossed.size(0)\n",
        "            correct_bin += (out_bin.argmax(1) == is_crossed).sum().item()\n",
        "\n",
        "            # Save binary predictions\n",
        "            y_true_binary.extend(is_crossed.cpu().numpy())\n",
        "            y_pred_binary.extend(out_bin.argmax(1).cpu().numpy())\n",
        "\n",
        "            # Only evaluate type classification for crossed images\n",
        "            mask = (is_crossed == 1)\n",
        "            if mask.any():\n",
        "                correct_type += (out_type[mask].argmax(1) == type_label[mask]).sum().item()\n",
        "                valid_type_total += mask.sum().item()\n",
        "\n",
        "                # Save type predictions\n",
        "                y_true_type.extend(type_label[mask].cpu().numpy())\n",
        "                y_pred_type.extend(out_type[mask].argmax(1).cpu().numpy())\n",
        "\n",
        "    bin_acc = correct_bin / total\n",
        "    type_acc = correct_type / valid_type_total if valid_type_total else 0.0\n",
        "\n",
        "    print(f\"Test Binary Accuracy (CLEAN vs CROSSED): {bin_acc:.4f}\")\n",
        "    print(f\"Test Type Accuracy (on crossed only): {type_acc:.4f}\")\n",
        "\n",
        "    # Plot Binary Confusion Matrix (CLEAN vs CROSSED)\n",
        "    cm_bin = confusion_matrix(y_true_binary, y_pred_binary)\n",
        "    disp_bin = ConfusionMatrixDisplay(confusion_matrix=cm_bin, display_labels=['CLEAN', 'CROSSED'])\n",
        "    fig, ax = plt.subplots(figsize=(6,6))\n",
        "    disp_bin.plot(ax=ax, cmap='Blues', values_format='.2g')\n",
        "    plt.title(\"Confusion Matrix - Binary (CLEAN vs CROSSED)\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Type Confusion Matrix (only crossed)\n",
        "    if y_true_type and y_pred_type:\n",
        "        all_labels = [\n",
        "            'MIXED', 'CROSS', 'DIAGONAL', 'DOUBLE_LINE',\n",
        "            'SCRATCH', 'SINGLE_LINE', 'WAVE', 'ZIG_ZAG'\n",
        "        ]\n",
        "\n",
        "        classes_in_data = np.unique(np.concatenate([y_true_type, y_pred_type]))\n",
        "        labels_used = [all_labels[i] for i in classes_in_data]\n",
        "\n",
        "        cm_type = confusion_matrix(y_true_type, y_pred_type, labels=classes_in_data)\n",
        "        disp_type = ConfusionMatrixDisplay(confusion_matrix=cm_type, display_labels=labels_used)\n",
        "        fig, ax = plt.subplots(figsize=(10,8))\n",
        "        disp_type.plot(ax=ax, cmap='Blues', values_format='.2g')\n",
        "        plt.title(\"Confusion Matrix - Crossed Types Only\")\n",
        "        plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    data_root = r\"D:\\PLUGGET\\D7047E\\Project\\Project\\data\\cross_out_dataset\"\n",
        "    trainloader, validationloader, testloader = get_data_loaders(\n",
        "        batch_size=64,\n",
        "        data_root=data_root,\n",
        "        sample_fraction=0.2,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    model = CrossoutResNet().to(device)\n",
        "    criterion_bin = nn.CrossEntropyLoss()\n",
        "    criterion_class = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    trained_model = train_model(\n",
        "        model,\n",
        "        criterion_bin,\n",
        "        criterion_class,\n",
        "        optimizer,\n",
        "        trainloader,\n",
        "        validationloader,\n",
        "        num_epochs=10,\n",
        "        device=device,\n",
        "        lambda_type=1.0\n",
        "    )\n",
        "\n",
        "    evaluate(trained_model, testloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Was ran on local machine so results are not here"
      ],
      "metadata": {
        "id": "24oQUPLAcpB8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HUMgBIsGcvmT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}